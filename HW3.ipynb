{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3: åŸºäºDueling DDQNçš„é…å¯¹äº¤æ˜“ç­–ç•¥\n",
    "\n",
    "**ä½œè€…**: å­¦ç”Ÿå§“å  \n",
    "**æ—¥æœŸ**: 2025-11-15  \n",
    "\n",
    "---\n",
    "\n",
    "## ä½œä¸šè¦æ±‚\n",
    "\n",
    "1. ä½¿ç”¨LLMå»ºè®®10ä¸ªDDQNæ¨¡å‹ç”¨äºé…å¯¹äº¤æ˜“\n",
    "2. é€‰æ‹©ä¸€ä¸ªæ¨¡å‹å¹¶ç”Ÿæˆä»£ç \n",
    "3. åˆ†å‰²æ•°æ®ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œå¹¶è®­ç»ƒæ¨¡å‹\n",
    "4. ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šç”Ÿæˆé¢„æµ‹\n",
    "5. è¯„ä¼°äº¤æ˜“ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: 10ä¸ªDDQNæ¨¡å‹å»ºè®®\n",
    "\n",
    "## æ¨¡å‹åˆ—è¡¨\n",
    "\n",
    "### 1. åŸºç¡€DDQN (Baseline DDQN)\n",
    "- **æ¶æ„**: 3å±‚å…¨è¿æ¥ç½‘ç»œ (128-64-32)\n",
    "- **ä¼˜ç‚¹**: å®ç°ç®€å•ï¼Œæ˜“äºè°ƒè¯•\n",
    "- **ç¼ºç‚¹**: ç‰¹å¾æå–èƒ½åŠ›æœ‰é™\n",
    "\n",
    "### 2. **Dueling DDQN** â­ (æœ€ç»ˆé€‰æ‹©)\n",
    "- **æ¶æ„**: åˆ†ç¦»ä»·å€¼æµå’Œä¼˜åŠ¿æµ\n",
    "- **ä¼˜ç‚¹**: æ›´å‡†ç¡®åœ°ä¼°è®¡çŠ¶æ€ä»·å€¼ï¼Œè®­ç»ƒç¨³å®š\n",
    "- **ç¼ºç‚¹**: ç½‘ç»œå¤æ‚åº¦ç¨é«˜\n",
    "- **é€‰æ‹©ç†ç”±**: æ€§èƒ½ä¸å¤æ‚åº¦çš„æœ€ä½³å¹³è¡¡\n",
    "\n",
    "### 3. Attention-based DDQN\n",
    "- **æ¶æ„**: å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶\n",
    "- **ä¼˜ç‚¹**: æ•æ‰é•¿æœŸä¾èµ–\n",
    "- **ç¼ºç‚¹**: è®­ç»ƒæ—¶é—´é•¿\n",
    "\n",
    "### 4. CNN-DDQN\n",
    "- **æ¶æ„**: 1Då·ç§¯å±‚æå–å±€éƒ¨ç‰¹å¾\n",
    "- **ä¼˜ç‚¹**: æå–ä»·æ ¼èµ°åŠ¿æ¨¡å¼\n",
    "- **ç¼ºç‚¹**: å¯èƒ½ä¸¢å¤±å…¨å±€ä¿¡æ¯\n",
    "\n",
    "### 5. LSTM-DDQN\n",
    "- **æ¶æ„**: åŒå±‚LSTM + å…¨è¿æ¥\n",
    "- **ä¼˜ç‚¹**: æ“…é•¿å¤„ç†æ—¶é—´åºåˆ—\n",
    "- **ç¼ºç‚¹**: è®­ç»ƒé€Ÿåº¦æ…¢\n",
    "\n",
    "### 6. Hierarchical DDQN\n",
    "- **æ¶æ„**: ä¸¤å±‚ç­–ç•¥ç½‘ç»œ\n",
    "- **ä¼˜ç‚¹**: åˆ†è§£å¤æ‚å†³ç­–\n",
    "- **ç¼ºç‚¹**: è®­ç»ƒå¤æ‚åº¦é«˜\n",
    "\n",
    "### 7. Ensemble DDQN\n",
    "- **æ¶æ„**: 3-5ä¸ªç‹¬ç«‹ç½‘ç»œé›†æˆ\n",
    "- **ä¼˜ç‚¹**: é™ä½æ–¹å·®ï¼Œæé«˜ç¨³å®šæ€§\n",
    "- **ç¼ºç‚¹**: è®¡ç®—æˆæœ¬é«˜\n",
    "\n",
    "### 8. Noisy DDQN\n",
    "- **æ¶æ„**: å‚æ•°åŒ–å™ªå£°å±‚\n",
    "- **ä¼˜ç‚¹**: æ¢ç´¢æ›´é«˜æ•ˆ\n",
    "- **ç¼ºç‚¹**: å®ç°ç¨å¤æ‚\n",
    "\n",
    "### 9. Prioritized Experience Replay DDQN\n",
    "- **æ¶æ„**: åŸºäºTDè¯¯å·®çš„ä¼˜å…ˆçº§é‡‡æ ·\n",
    "- **ä¼˜ç‚¹**: æ›´é«˜æ•ˆåˆ©ç”¨ç»éªŒ\n",
    "- **ç¼ºç‚¹**: å†…å­˜å¼€é”€å¤§\n",
    "\n",
    "### 10. Rainbow DQN\n",
    "- **æ¶æ„**: æ•´åˆå¤šç§æ”¹è¿›æŠ€æœ¯\n",
    "- **ä¼˜ç‚¹**: æ€§èƒ½æœ€ä¼˜\n",
    "- **ç¼ºç‚¹**: å®ç°éš¾åº¦æœ€é«˜\n",
    "\n",
    "---\n",
    "\n",
    "## æœ€ç»ˆé€‰æ‹©: Dueling DDQN\n",
    "\n",
    "**ç†ç”±:**\n",
    "1. é€šè¿‡åˆ†ç¦»çŠ¶æ€ä»·å€¼V(s)å’ŒåŠ¨ä½œä¼˜åŠ¿A(s,a)ï¼Œæ›´å‡†ç¡®è¯„ä¼°æ¯ä¸ªåŠ¨ä½œçš„ä»·å€¼\n",
    "2. åœ¨é…å¯¹äº¤æ˜“çš„ä¹°å…¥/å–å‡º/æŒæœ‰å†³ç­–ä¸­è¡¨ç°ä¼˜å¼‚\n",
    "3. å®ç°å¤æ‚åº¦é€‚ä¸­ï¼Œè®­ç»ƒç¨³å®š\n",
    "4. æ˜“äºæ‰©å±•å’Œæ”¹è¿›\n",
    "\n",
    "**æ ¸å¿ƒå…¬å¼:**\n",
    "```\n",
    "Q(s,a) = V(s) + (A(s,a) - mean(A(s,a)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: ç¯å¢ƒé…ç½®\n",
    "\n",
    "## è®­ç»ƒæ¨¡å¼é€‰æ‹©\n",
    "\n",
    "- **å¿«é€Ÿæ¨¡å¼**: 50 episodesï¼Œé€‚åˆå¿«é€ŸéªŒè¯å’Œæ¼”ç¤ºï¼ˆçº¦5-10åˆ†é’Ÿï¼‰\n",
    "- **å®Œæ•´æ¨¡å¼**: 500 episodesï¼Œå®Œæ•´è®­ç»ƒä»¥è·å¾—æœ€ä½³æ€§èƒ½ï¼ˆçº¦30-60åˆ†é’Ÿï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš¡ å¿«é€Ÿæ¨¡å¼: 50 episodes, 500 æ•°æ®æ ·æœ¬\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31måœ¨å½“å‰å•å…ƒæ ¼æˆ–ä¸Šä¸€ä¸ªå•å…ƒæ ¼ä¸­æ‰§è¡Œä»£ç æ—¶ Kernel å´©æºƒã€‚\n",
      "\u001b[1;31mè¯·æŸ¥çœ‹å•å…ƒæ ¼ä¸­çš„ä»£ç ï¼Œä»¥ç¡®å®šæ•…éšœçš„å¯èƒ½åŸå› ã€‚\n",
      "\u001b[1;31må•å‡»<a href='https://aka.ms/vscodeJupyterKernelCrash'>æ­¤å¤„</a>äº†è§£è¯¦ç»†ä¿¡æ¯ã€‚\n",
      "\u001b[1;31mæœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ Jupyter <a href='command:jupyter.viewOutput'>log</a>ã€‚"
     ]
    }
   ],
   "source": [
    "# ========== é…ç½®å‚æ•° ==========\n",
    "QUICK_MODE = True  # True=å¿«é€Ÿæ¨¡å¼(50 episodes), False=å®Œæ•´æ¨¡å¼(500 episodes)\n",
    "\n",
    "# æ ¹æ®æ¨¡å¼è®¾ç½®å‚æ•°\n",
    "if QUICK_MODE:\n",
    "    N_EPISODES = 50\n",
    "    N_SAMPLES = 500\n",
    "    print(\"âš¡ å¿«é€Ÿæ¨¡å¼: 50 episodes, 500 æ•°æ®æ ·æœ¬\")\n",
    "else:\n",
    "    N_EPISODES = 500\n",
    "    N_SAMPLES = 1000\n",
    "    print(\"ğŸ”¥ å®Œæ•´æ¨¡å¼: 500 episodes, 1000 æ•°æ®æ ·æœ¬\")\n",
    "\n",
    "# å…¶ä»–è¶…å‚æ•°\n",
    "LEARNING_RATE = 1e-3\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "TEST_SIZE = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å®‰è£…ä¾èµ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥åŸºç¡€åº“\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# è®¾ç½®ç»˜å›¾æ ·å¼\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# æ£€æµ‹è®¾å¤‡\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"ä½¿ç”¨è®¾å¤‡: {device}\")\n",
    "print(\"âœ“ æ‰€æœ‰åº“å¯¼å…¥æˆåŠŸ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: æ ¸å¿ƒä»£ç å®ç°\n",
    "\n",
    "## 3.1 Dueling DDQNæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(nn.Module):\n",
    "    \"\"\"Dueling DQNç½‘ç»œæ¶æ„\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        \n",
    "        # å…±äº«ç‰¹å¾æå–å±‚\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # çŠ¶æ€ä»·å€¼æµ V(s)\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 4, 1)\n",
    "        )\n",
    "        \n",
    "        # åŠ¨ä½œä¼˜åŠ¿æµ A(s, a)\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 4, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"Q(s,a) = V(s) + (A(s,a) - mean(A(s,a)))\"\"\"\n",
    "        features = self.feature(state)\n",
    "        value = self.value_stream(features)\n",
    "        advantage = self.advantage_stream(features)\n",
    "        \n",
    "        # ç»„åˆä»·å€¼å’Œä¼˜åŠ¿\n",
    "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
    "        return q_values\n",
    "\n",
    "print(\"âœ“ Dueling DQNæ¨¡å‹å®šä¹‰å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"ç»éªŒå›æ”¾ç¼“å†²åŒº\"\"\"\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        batch = [self.buffer[idx] for idx in indices]\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards),\n",
    "            np.array(next_states),\n",
    "            np.array(dones)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "print(\"âœ“ ç»éªŒå›æ”¾ç¼“å†²åŒºå®šä¹‰å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDDQNAgent:\n",
    "    \"\"\"Dueling DDQNæ™ºèƒ½ä½“\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, learning_rate=1e-3, gamma=0.99, \n",
    "                 epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995,\n",
    "                 buffer_capacity=10000, batch_size=64, target_update_freq=10, device='cpu'):\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.device = device\n",
    "        \n",
    "        # åˆ›å»ºä¸»ç½‘ç»œå’Œç›®æ ‡ç½‘ç»œ\n",
    "        self.policy_net = DuelingDQN(state_dim, action_dim).to(device)\n",
    "        self.target_net = DuelingDQN(state_dim, action_dim).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        \n",
    "        # ä¼˜åŒ–å™¨\n",
    "        self.optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # ç»éªŒå›æ”¾\n",
    "        self.replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "        \n",
    "        # è®­ç»ƒè®¡æ•°\n",
    "        self.train_step = 0\n",
    "    \n",
    "    def select_action(self, state, training=True):\n",
    "        \"\"\"é€‰æ‹©åŠ¨ä½œ (epsilon-greedy)\"\"\"\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.action_dim)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            q_values = self.policy_net(state_tensor)\n",
    "            return q_values.argmax(dim=1).item()\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"æ›´æ–°ç½‘ç»œ\"\"\"\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        # é‡‡æ ·æ‰¹æ¬¡\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        # è½¬æ¢ä¸ºå¼ é‡\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "        \n",
    "        # è®¡ç®—å½“å‰Qå€¼\n",
    "        current_q_values = self.policy_net(states).gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        # è®¡ç®—ç›®æ ‡Qå€¼ (Double DQN)\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.policy_net(next_states).argmax(dim=1, keepdim=True)\n",
    "            next_q_values = self.target_net(next_states).gather(1, next_actions)\n",
    "            target_q_values = rewards.unsqueeze(1) + (1 - dones.unsqueeze(1)) * self.gamma * next_q_values\n",
    "        \n",
    "        # è®¡ç®—æŸå¤±\n",
    "        loss = F.mse_loss(current_q_values, target_q_values)\n",
    "        \n",
    "        # ä¼˜åŒ–\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # æ›´æ–°epsilon\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "        \n",
    "        # å®šæœŸæ›´æ–°ç›®æ ‡ç½‘ç»œ\n",
    "        self.train_step += 1\n",
    "        if self.train_step % self.target_update_freq == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "print(\"âœ“ Dueling DDQNæ™ºèƒ½ä½“å®šä¹‰å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 é…å¯¹äº¤æ˜“ç¯å¢ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairTradingEnv:\n",
    "    \"\"\"é…å¯¹äº¤æ˜“ç¯å¢ƒ\"\"\"\n",
    "    def __init__(self, data, initial_balance=10000.0, transaction_cost=0.001, \n",
    "                 max_position=1, window_size=20):\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.initial_balance = initial_balance\n",
    "        self.transaction_cost = transaction_cost\n",
    "        self.max_position = max_position\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        # è®¡ç®—ç‰¹å¾\n",
    "        self._calculate_features()\n",
    "        \n",
    "        # åŠ¨ä½œç©ºé—´: 0=å¹³ä»“, 1=åšå¤š, 2=åšç©º, 3=æŒæœ‰\n",
    "        self.action_space_n = 4\n",
    "        self.state_dim = self._get_state(self.window_size).shape[0]\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def _calculate_features(self):\n",
    "        \"\"\"è®¡ç®—æŠ€æœ¯ç‰¹å¾\"\"\"\n",
    "        self.data['spread'] = self.data['price_A'] - self.data['price_B']\n",
    "        self.data['spread_ma'] = self.data['spread'].rolling(window=self.window_size).mean()\n",
    "        self.data['spread_std'] = self.data['spread'].rolling(window=self.window_size).std()\n",
    "        self.data['z_score'] = (self.data['spread'] - self.data['spread_ma']) / (self.data['spread_std'] + 1e-8)\n",
    "        self.data['spread_return'] = self.data['spread'].pct_change()\n",
    "        self.data = self.data.fillna(0)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_step = self.window_size\n",
    "        self.balance = self.initial_balance\n",
    "        self.position = 0\n",
    "        self.entry_price = 0.0\n",
    "        self.total_trades = 0\n",
    "        self.winning_trades = 0\n",
    "        self.equity_curve = [self.initial_balance]\n",
    "        return self._get_state(self.current_step)\n",
    "    \n",
    "    def _get_state(self, step):\n",
    "        \"\"\"è·å–çŠ¶æ€\"\"\"\n",
    "        if step < self.window_size:\n",
    "            step = self.window_size\n",
    "        \n",
    "        current_spread = self.data.loc[step, 'spread']\n",
    "        z_score = self.data.loc[step, 'z_score']\n",
    "        spread_ma = self.data.loc[step, 'spread_ma']\n",
    "        spread_std = self.data.loc[step, 'spread_std']\n",
    "        \n",
    "        spread_history = self.data.loc[max(0, step-5):step, 'spread'].values\n",
    "        if len(spread_history) < 6:\n",
    "            spread_history = np.pad(spread_history, (6-len(spread_history), 0), 'constant')\n",
    "        \n",
    "        if spread_std > 0:\n",
    "            spread_history_norm = (spread_history - spread_ma) / spread_std\n",
    "        else:\n",
    "            spread_history_norm = spread_history\n",
    "        \n",
    "        position_feature = self.position / self.max_position\n",
    "        \n",
    "        if self.position != 0:\n",
    "            unrealized_pnl = (current_spread - self.entry_price) * self.position\n",
    "            unrealized_pnl_pct = unrealized_pnl / self.balance\n",
    "        else:\n",
    "            unrealized_pnl_pct = 0.0\n",
    "        \n",
    "        state = np.array([\n",
    "            z_score,\n",
    "            spread_std,\n",
    "            position_feature,\n",
    "            unrealized_pnl_pct,\n",
    "            *spread_history_norm\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"æ‰§è¡ŒåŠ¨ä½œ\"\"\"\n",
    "        current_spread = self.data.loc[self.current_step, 'spread']\n",
    "        reward = 0.0\n",
    "        trade_executed = False\n",
    "        \n",
    "        # åŠ¨ä½œæ‰§è¡Œé€»è¾‘\n",
    "        if action == 0:  # å¹³ä»“\n",
    "            if self.position != 0:\n",
    "                pnl = (current_spread - self.entry_price) * self.position\n",
    "                cost = abs(self.position) * self.transaction_cost * self.balance\n",
    "                net_pnl = pnl - cost\n",
    "                self.balance += net_pnl\n",
    "                reward = net_pnl / self.initial_balance\n",
    "                self.total_trades += 1\n",
    "                if net_pnl > 0:\n",
    "                    self.winning_trades += 1\n",
    "                self.position = 0\n",
    "                self.entry_price = 0.0\n",
    "                trade_executed = True\n",
    "        \n",
    "        elif action == 1:  # åšå¤š\n",
    "            if self.position <= 0:\n",
    "                if self.position < 0:\n",
    "                    pnl = (current_spread - self.entry_price) * self.position\n",
    "                    cost = abs(self.position) * self.transaction_cost * self.balance\n",
    "                    self.balance += pnl - cost\n",
    "                self.position = self.max_position\n",
    "                self.entry_price = current_spread\n",
    "                cost = self.transaction_cost * self.balance\n",
    "                self.balance -= cost\n",
    "                trade_executed = True\n",
    "        \n",
    "        elif action == 2:  # åšç©º\n",
    "            if self.position >= 0:\n",
    "                if self.position > 0:\n",
    "                    pnl = (current_spread - self.entry_price) * self.position\n",
    "                    cost = abs(self.position) * self.transaction_cost * self.balance\n",
    "                    self.balance += pnl - cost\n",
    "                self.position = -self.max_position\n",
    "                self.entry_price = current_spread\n",
    "                cost = self.transaction_cost * self.balance\n",
    "                self.balance -= cost\n",
    "                trade_executed = True\n",
    "        \n",
    "        # æŒä»“æµ®åŠ¨ç›ˆäº\n",
    "        if self.position != 0 and not trade_executed:\n",
    "            unrealized_pnl = (current_spread - self.entry_price) * self.position\n",
    "            reward = unrealized_pnl / self.initial_balance * 0.1\n",
    "        \n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.data) - 1\n",
    "        \n",
    "        # å¼ºåˆ¶å¹³ä»“\n",
    "        if done and self.position != 0:\n",
    "            final_spread = self.data.loc[self.current_step, 'spread']\n",
    "            pnl = (final_spread - self.entry_price) * self.position\n",
    "            cost = abs(self.position) * self.transaction_cost * self.balance\n",
    "            self.balance += pnl - cost\n",
    "            self.position = 0\n",
    "        \n",
    "        self.equity_curve.append(self.balance)\n",
    "        next_state = self._get_state(self.current_step) if not done else np.zeros(self.state_dim)\n",
    "        \n",
    "        info = {\n",
    "            'balance': self.balance,\n",
    "            'position': self.position,\n",
    "            'total_trades': self.total_trades,\n",
    "            'winning_trades': self.winning_trades,\n",
    "            'trade_executed': trade_executed\n",
    "        }\n",
    "        \n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def get_performance_metrics(self):\n",
    "        \"\"\"è®¡ç®—æ€§èƒ½æŒ‡æ ‡\"\"\"\n",
    "        equity_curve = np.array(self.equity_curve)\n",
    "        returns = np.diff(equity_curve) / equity_curve[:-1]\n",
    "        \n",
    "        total_return = (self.balance - self.initial_balance) / self.initial_balance\n",
    "        sharpe_ratio = np.mean(returns) / (np.std(returns) + 1e-8) * np.sqrt(252)\n",
    "        \n",
    "        cummax = np.maximum.accumulate(equity_curve)\n",
    "        drawdown = (equity_curve - cummax) / cummax\n",
    "        max_drawdown = np.min(drawdown)\n",
    "        \n",
    "        win_rate = self.winning_trades / self.total_trades if self.total_trades > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'total_return': total_return,\n",
    "            'sharpe_ratio': sharpe_ratio,\n",
    "            'max_drawdown': max_drawdown,\n",
    "            'win_rate': win_rate,\n",
    "            'total_trades': self.total_trades,\n",
    "            'final_balance': self.balance\n",
    "        }\n",
    "\n",
    "print(\"âœ“ é…å¯¹äº¤æ˜“ç¯å¢ƒå®šä¹‰å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 æ•°æ®ç”Ÿæˆå’Œå¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_pair_data(n_samples=1000, mean_reversion_speed=0.1, \n",
    "                                volatility=0.02, random_seed=42):\n",
    "    \"\"\"ç”Ÿæˆåˆæˆçš„é…å¯¹äº¤æ˜“æ•°æ®\"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    base_price = 100.0\n",
    "    price_a = np.zeros(n_samples)\n",
    "    price_b = np.zeros(n_samples)\n",
    "    \n",
    "    price_a[0] = base_price\n",
    "    price_b[0] = base_price\n",
    "    \n",
    "    spread_mean = 0.0\n",
    "    spread = np.zeros(n_samples)\n",
    "    \n",
    "    for t in range(1, n_samples):\n",
    "        spread[t] = (spread[t-1] * (1 - mean_reversion_speed) + \n",
    "                    spread_mean * mean_reversion_speed + \n",
    "                    np.random.normal(0, volatility))\n",
    "        \n",
    "        common_shock = np.random.normal(0, volatility)\n",
    "        price_a[t] = price_a[t-1] * (1 + common_shock + np.random.normal(0, volatility * 0.5))\n",
    "        price_b[t] = price_a[t] - spread[t]\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'price_A': price_a,\n",
    "        'price_B': price_b\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def split_train_test(data, test_size=0.2):\n",
    "    \"\"\"æ—¶é—´åºåˆ—é¡ºåºåˆ†å‰²\"\"\"\n",
    "    split_idx = int(len(data) * (1 - test_size))\n",
    "    train_data = data.iloc[:split_idx].reset_index(drop=True)\n",
    "    test_data = data.iloc[split_idx:].reset_index(drop=True)\n",
    "    return train_data, test_data\n",
    "\n",
    "print(\"âœ“ æ•°æ®å¤„ç†å‡½æ•°å®šä¹‰å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: æ•°æ®å‡†å¤‡ï¼ˆä»»åŠ¡3 - æ•°æ®åˆ†å‰²ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆæ•°æ®\n",
    "print(f\"ç”Ÿæˆ {N_SAMPLES} ä¸ªäº¤æ˜“æ—¥çš„é…å¯¹æ•°æ®...\")\n",
    "data = generate_synthetic_pair_data(n_samples=N_SAMPLES)\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "axes[0].plot(data['price_A'], label='Asset A', linewidth=2)\n",
    "axes[0].plot(data['price_B'], label='Asset B', linewidth=2)\n",
    "axes[0].set_title('èµ„äº§ä»·æ ¼åºåˆ—', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('ä»·æ ¼')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "spread = data['price_A'] - data['price_B']\n",
    "axes[1].plot(spread, color='purple', linewidth=2)\n",
    "axes[1].axhline(y=spread.mean(), color='r', linestyle='--', label='å‡å€¼')\n",
    "axes[1].fill_between(range(len(spread)), \n",
    "                      spread.mean() - spread.std(), \n",
    "                      spread.mean() + spread.std(), \n",
    "                      alpha=0.3, color='red', label='Â±1Ïƒ')\n",
    "axes[1].set_title('ä»·å·®åºåˆ—ï¼ˆå‡å€¼å›å½’ç‰¹æ€§ï¼‰', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('æ—¶é—´')\n",
    "axes[1].set_ylabel('ä»·å·®')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ“ æ•°æ®ç”Ÿæˆå®Œæˆ: {len(data)} ä¸ªäº¤æ˜“æ—¥\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "train_data, test_data = split_train_test(data, test_size=TEST_SIZE)\n",
    "\n",
    "print(f\"è®­ç»ƒé›†å¤§å°: {len(train_data)} ä¸ªäº¤æ˜“æ—¥ ({(1-TEST_SIZE)*100:.0f}%)\")\n",
    "print(f\"æµ‹è¯•é›†å¤§å°: {len(test_data)} ä¸ªäº¤æ˜“æ—¥ ({TEST_SIZE*100:.0f}%)\")\n",
    "print(\"âœ“ æ•°æ®åˆ†å‰²å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: æ¨¡å‹è®­ç»ƒï¼ˆä»»åŠ¡3 - è®­ç»ƒæ¨¡å‹ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºç¯å¢ƒè·å–ç»´åº¦\n",
    "temp_env = PairTradingEnv(train_data)\n",
    "state_dim = temp_env.state_dim\n",
    "action_dim = temp_env.action_space_n\n",
    "\n",
    "print(f\"çŠ¶æ€ç»´åº¦: {state_dim}\")\n",
    "print(f\"åŠ¨ä½œç»´åº¦: {action_dim}\")\n",
    "\n",
    "# åˆ›å»ºæ™ºèƒ½ä½“\n",
    "agent = DuelingDDQNAgent(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    gamma=GAMMA,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_end=0.01,\n",
    "    epsilon_decay=0.995,\n",
    "    buffer_capacity=BUFFER_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_update_freq=10,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"âœ“ æ™ºèƒ½ä½“åˆ›å»ºå®Œæˆ\")\n",
    "print(f\"è®¾å¤‡: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒ\n",
    "print(f\"\\nå¼€å§‹è®­ç»ƒ ({N_EPISODES} episodes)...\\n\")\n",
    "\n",
    "episode_rewards = []\n",
    "performance_metrics = []\n",
    "losses = []\n",
    "best_reward = -np.inf\n",
    "\n",
    "for episode in tqdm(range(N_EPISODES), desc=\"è®­ç»ƒè¿›åº¦\"):\n",
    "    env = PairTradingEnv(train_data)\n",
    "    state = env.reset()\n",
    "    \n",
    "    episode_reward = 0\n",
    "    episode_loss = []\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.select_action(state, training=True)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        agent.store_transition(state, action, reward, next_state, done)\n",
    "        \n",
    "        loss = agent.update()\n",
    "        if loss is not None:\n",
    "            episode_loss.append(loss)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        state = next_state\n",
    "    \n",
    "    episode_rewards.append(episode_reward)\n",
    "    losses.append(np.mean(episode_loss) if episode_loss else 0)\n",
    "    metrics = env.get_performance_metrics()\n",
    "    performance_metrics.append(metrics)\n",
    "    \n",
    "    # å®šæœŸæ‰“å°\n",
    "    if (episode + 1) % max(10, N_EPISODES // 10) == 0:\n",
    "        recent_rewards = episode_rewards[-10:]\n",
    "        recent_returns = [m['total_return'] for m in performance_metrics[-10:]]\n",
    "        recent_sharpe = [m['sharpe_ratio'] for m in performance_metrics[-10:]]\n",
    "        \n",
    "        print(f\"\\nEpisode {episode + 1}/{N_EPISODES}\")\n",
    "        print(f\"  å¹³å‡å¥–åŠ±: {np.mean(recent_rewards):.4f}\")\n",
    "        print(f\"  å¹³å‡æ”¶ç›Šç‡: {np.mean(recent_returns)*100:.2f}%\")\n",
    "        print(f\"  å¹³å‡å¤æ™®æ¯”ç‡: {np.mean(recent_sharpe):.4f}\")\n",
    "        print(f\"  Epsilon: {agent.epsilon:.4f}\")\n",
    "    \n",
    "    # ä¿å­˜æœ€ä½³æ¨¡å‹\n",
    "    if episode_reward > best_reward:\n",
    "        best_reward = episode_reward\n",
    "        best_episode = episode + 1\n",
    "\n",
    "print(f\"\\nâœ“ è®­ç»ƒå®Œæˆ!\")\n",
    "print(f\"æœ€ä½³episode: {best_episode}, å¥–åŠ±: {best_reward:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Episodeå¥–åŠ±\n",
    "axes[0, 0].plot(episode_rewards, alpha=0.6, label='Episodeå¥–åŠ±')\n",
    "axes[0, 0].plot(pd.Series(episode_rewards).rolling(10).mean(), label='10æœŸç§»åŠ¨å¹³å‡', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('æ€»å¥–åŠ±')\n",
    "axes[0, 0].set_title('è®­ç»ƒå¥–åŠ±æ›²çº¿')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# æ”¶ç›Šç‡\n",
    "returns = [m['total_return'] for m in performance_metrics]\n",
    "axes[0, 1].plot(returns, alpha=0.6, label='æ€»æ”¶ç›Šç‡')\n",
    "axes[0, 1].plot(pd.Series(returns).rolling(10).mean(), label='10æœŸç§»åŠ¨å¹³å‡', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Episode')\n",
    "axes[0, 1].set_ylabel('æ”¶ç›Šç‡')\n",
    "axes[0, 1].set_title('æ€»æ”¶ç›Šç‡æ›²çº¿')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "\n",
    "# å¤æ™®æ¯”ç‡\n",
    "sharpe_ratios = [m['sharpe_ratio'] for m in performance_metrics]\n",
    "axes[1, 0].plot(sharpe_ratios, alpha=0.6, label='å¤æ™®æ¯”ç‡')\n",
    "axes[1, 0].plot(pd.Series(sharpe_ratios).rolling(10).mean(), label='10æœŸç§»åŠ¨å¹³å‡', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Episode')\n",
    "axes[1, 0].set_ylabel('å¤æ™®æ¯”ç‡')\n",
    "axes[1, 0].set_title('å¤æ™®æ¯”ç‡æ›²çº¿')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "\n",
    "# æŸå¤±\n",
    "axes[1, 1].plot(losses, alpha=0.6, label='è®­ç»ƒæŸå¤±')\n",
    "axes[1, 1].plot(pd.Series(losses).rolling(10).mean(), label='10æœŸç§»åŠ¨å¹³å‡', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Episode')\n",
    "axes[1, 1].set_ylabel('æŸå¤±')\n",
    "axes[1, 1].set_title('è®­ç»ƒæŸå¤±æ›²çº¿')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: æµ‹è¯•é›†é¢„æµ‹ï¼ˆä»»åŠ¡4ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åœ¨æµ‹è¯•é›†ä¸Šè¿è¡Œ\n",
    "print(\"åœ¨æµ‹è¯•é›†ä¸Šç”Ÿæˆé¢„æµ‹...\")\n",
    "\n",
    "test_env = PairTradingEnv(test_data)\n",
    "state = test_env.reset()\n",
    "done = False\n",
    "step = 0\n",
    "\n",
    "trade_history = []\n",
    "\n",
    "while not done:\n",
    "    # ä½¿ç”¨è®­ç»ƒå¥½çš„ç­–ç•¥ï¼ˆä¸æ¢ç´¢ï¼‰\n",
    "    action = agent.select_action(state, training=False)\n",
    "    next_state, reward, done, info = test_env.step(action)\n",
    "    \n",
    "    trade_history.append({\n",
    "        'step': step,\n",
    "        'action': action,\n",
    "        'position': info['position'],\n",
    "        'balance': info['balance'],\n",
    "        'reward': reward,\n",
    "        'trade_executed': info['trade_executed']\n",
    "    })\n",
    "    \n",
    "    state = next_state\n",
    "    step += 1\n",
    "\n",
    "# è½¬æ¢ä¸ºDataFrame\n",
    "df_history = pd.DataFrame(trade_history)\n",
    "\n",
    "print(f\"âœ“ æµ‹è¯•å®Œæˆï¼Œå…± {len(trade_history)} ä¸ªæ—¶é—´æ­¥\")\n",
    "print(f\"\\nå‰10è¡Œäº¤æ˜“è®°å½•:\")\n",
    "print(df_history.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 7: äº¤æ˜“ç»“æœè¯„ä¼°ï¼ˆä»»åŠ¡5ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¡ç®—æ€§èƒ½æŒ‡æ ‡\n",
    "metrics = test_env.get_performance_metrics()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"æµ‹è¯•é›†è¯„ä¼°ç»“æœ\")\n",
    "print(\"=\"*60)\n",
    "print(f\"æ€»æ”¶ç›Šç‡:       {metrics['total_return']*100:.2f}%\")\n",
    "print(f\"å¤æ™®æ¯”ç‡:       {metrics['sharpe_ratio']:.4f}\")\n",
    "print(f\"æœ€å¤§å›æ’¤:       {metrics['max_drawdown']*100:.2f}%\")\n",
    "print(f\"æ€»äº¤æ˜“æ¬¡æ•°:     {metrics['total_trades']}\")\n",
    "print(f\"ç›ˆåˆ©äº¤æ˜“æ¬¡æ•°:   {int(metrics['win_rate'] * metrics['total_trades'])}\")\n",
    "print(f\"èƒœç‡:           {metrics['win_rate']*100:.2f}%\")\n",
    "print(f\"åˆå§‹èµ„é‡‘:       $10,000.00\")\n",
    "print(f\"æœ€ç»ˆèµ„é‡‘:       ${metrics['final_balance']:.2f}\")\n",
    "print(f\"å‡€æ”¶ç›Š:         ${metrics['final_balance'] - 10000:.2f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¯¦ç»†è¯„ä¼°æŠ¥å‘Š\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. æƒç›Šæ›²çº¿\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "equity = np.array(test_env.equity_curve)\n",
    "ax1.plot(equity, linewidth=2, label='æƒç›Šæ›²çº¿', color='steelblue')\n",
    "ax1.axhline(y=10000, color='r', linestyle='--', alpha=0.5, label='åˆå§‹èµ„é‡‘')\n",
    "ax1.fill_between(range(len(equity)), 10000, equity, alpha=0.3)\n",
    "ax1.set_xlabel('æ—¶é—´æ­¥', fontsize=12)\n",
    "ax1.set_ylabel('è´¦æˆ·æƒç›Š ($)', fontsize=12)\n",
    "ax1.set_title('æµ‹è¯•é›†æƒç›Šæ›²çº¿', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. å›æ’¤æ›²çº¿\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "cummax = np.maximum.accumulate(equity)\n",
    "drawdown = (equity - cummax) / cummax * 100\n",
    "ax2.fill_between(range(len(drawdown)), 0, drawdown, color='red', alpha=0.5)\n",
    "ax2.plot(drawdown, color='darkred', linewidth=1.5)\n",
    "ax2.set_xlabel('æ—¶é—´æ­¥', fontsize=12)\n",
    "ax2.set_ylabel('å›æ’¤ (%)', fontsize=12)\n",
    "ax2.set_title(f'å›æ’¤åˆ†æ (æœ€å¤§å›æ’¤: {metrics[\"max_drawdown\"]*100:.2f}%)', fontsize=13, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. æ”¶ç›Šåˆ†å¸ƒ\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "returns = np.diff(equity) / equity[:-1] * 100\n",
    "ax3.hist(returns, bins=30, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "ax3.axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "ax3.axvline(x=np.mean(returns), color='g', linestyle='--', \n",
    "           linewidth=2, label=f'å‡å€¼: {np.mean(returns):.4f}%')\n",
    "ax3.set_xlabel('æ”¶ç›Šç‡ (%)', fontsize=12)\n",
    "ax3.set_ylabel('é¢‘æ•°', fontsize=12)\n",
    "ax3.set_title('æ”¶ç›Šç‡åˆ†å¸ƒ', fontsize=13, fontweight='bold')\n",
    "ax3.legend(fontsize=11)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. æŒä»“åˆ†æ\n",
    "ax4 = fig.add_subplot(gs[2, 0])\n",
    "position_counts = df_history['position'].value_counts()\n",
    "colors = ['gray', 'green', 'red']\n",
    "labels = ['ç©ºä»“ (0)', 'åšå¤š (1)', 'åšç©º (-1)']\n",
    "positions_to_show = []\n",
    "counts_to_show = []\n",
    "colors_to_show = []\n",
    "for i, pos in enumerate([0, 1, -1]):\n",
    "    if pos in position_counts.index:\n",
    "        positions_to_show.append(labels[i])\n",
    "        counts_to_show.append(position_counts[pos])\n",
    "        colors_to_show.append(colors[i])\n",
    "ax4.bar(range(len(counts_to_show)), counts_to_show, color=colors_to_show, alpha=0.7)\n",
    "ax4.set_xticks(range(len(positions_to_show)))\n",
    "ax4.set_xticklabels(positions_to_show)\n",
    "ax4.set_ylabel('æ—¶é—´æ­¥æ•°', fontsize=12)\n",
    "ax4.set_title('æŒä»“åˆ†å¸ƒ', fontsize=13, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 5. åŠ¨ä½œåˆ†å¸ƒ\n",
    "ax5 = fig.add_subplot(gs[2, 1])\n",
    "action_counts = df_history['action'].value_counts().sort_index()\n",
    "action_labels = ['å¹³ä»“', 'åšå¤š', 'åšç©º', 'æŒæœ‰']\n",
    "action_colors = ['gray', 'green', 'red', 'blue']\n",
    "bars = ax5.bar(range(len(action_counts)), action_counts.values,\n",
    "              color=[action_colors[i] for i in action_counts.index], alpha=0.7)\n",
    "ax5.set_xticks(range(len(action_counts)))\n",
    "ax5.set_xticklabels([action_labels[i] for i in action_counts.index])\n",
    "ax5.set_ylabel('æ‰§è¡Œæ¬¡æ•°', fontsize=12)\n",
    "ax5.set_title('åŠ¨ä½œåˆ†å¸ƒ', fontsize=13, fontweight='bold')\n",
    "ax5.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax5.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height)}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.suptitle('Dueling DDQNé…å¯¹äº¤æ˜“ç­–ç•¥ - æµ‹è¯•é›†è¯„ä¼°æŠ¥å‘Š',\n",
    "            fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ è¯„ä¼°æŠ¥å‘Šç”Ÿæˆå®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 8: æ€»ç»“\n",
    "\n",
    "## ä½œä¸šå®Œæˆæƒ…å†µ\n",
    "\n",
    "âœ… **ä»»åŠ¡1**: ä½¿ç”¨LLMå»ºè®®äº†10ä¸ªDDQNæ¨¡å‹ï¼Œè¯¦è§Part 1  \n",
    "âœ… **ä»»åŠ¡2**: é€‰æ‹©äº†Dueling DDQNå¹¶å®ç°å®Œæ•´ä»£ç   \n",
    "âœ… **ä»»åŠ¡3**: åˆ†å‰²æ•°æ®ä¸ºè®­ç»ƒé›†(80%)å’Œæµ‹è¯•é›†(20%)ï¼Œå¹¶è®­ç»ƒæ¨¡å‹  \n",
    "âœ… **ä»»åŠ¡4**: åœ¨æµ‹è¯•é›†ä¸Šç”Ÿæˆäº¤æ˜“é¢„æµ‹  \n",
    "âœ… **ä»»åŠ¡5**: å®Œæˆäº¤æ˜“ç»“æœè¯„ä¼°ï¼ŒåŒ…æ‹¬å¤šç»´åº¦æŒ‡æ ‡å’Œå¯è§†åŒ–æŠ¥å‘Š  \n",
    "\n",
    "## æŠ€æœ¯æ€»ç»“\n",
    "\n",
    "### é€‰æ‹©çš„æ¨¡å‹: Dueling DDQN\n",
    "- **æ ¸å¿ƒæ€æƒ³**: åˆ†ç¦»çŠ¶æ€ä»·å€¼V(s)å’ŒåŠ¨ä½œä¼˜åŠ¿A(s,a)\n",
    "- **æ•°å­¦å…¬å¼**: Q(s,a) = V(s) + (A(s,a) - mean(A(s,a)))\n",
    "- **ä¸»è¦ä¼˜åŠ¿**: æ›´å‡†ç¡®çš„Qå€¼ä¼°è®¡ï¼Œè®­ç»ƒæ›´ç¨³å®š\n",
    "\n",
    "### å®ç°ç»†èŠ‚\n",
    "- **çŠ¶æ€ç©ºé—´**: 11ç»´ï¼ˆz-score, ä»·å·®ç»Ÿè®¡, æŒä»“, æµ®ç›ˆ, å†å²ä»·å·®ï¼‰\n",
    "- **åŠ¨ä½œç©ºé—´**: 4ä¸ªç¦»æ•£åŠ¨ä½œï¼ˆå¹³ä»“/åšå¤š/åšç©º/æŒæœ‰ï¼‰\n",
    "- **å¥–åŠ±è®¾è®¡**: å®é™…ç›ˆäº + æµ®åŠ¨ç›ˆäºï¼ˆå°æƒé‡ï¼‰\n",
    "- **ç¯å¢ƒç‰¹æ€§**: çœŸå®äº¤æ˜“æˆæœ¬å»ºæ¨¡ï¼ˆ0.1%ï¼‰\n",
    "\n",
    "### æ€§èƒ½è¡¨ç°\n",
    "æ ¹æ®ä¸Šè¿°è¯„ä¼°ç»“æœï¼š\n",
    "- æ€»æ”¶ç›Šç‡ã€å¤æ™®æ¯”ç‡ã€æœ€å¤§å›æ’¤ç­‰æŒ‡æ ‡\n",
    "- äº¤æ˜“é¢‘ç‡å’Œèƒœç‡\n",
    "- ç­–ç•¥çš„ç¨³å®šæ€§å’Œé£é™©æ§åˆ¶èƒ½åŠ›\n",
    "\n",
    "## æ”¹è¿›æ–¹å‘\n",
    "1. æ·»åŠ ä¼˜å…ˆç»éªŒå›æ”¾(PER)æé«˜æ ·æœ¬åˆ©ç”¨æ•ˆç‡\n",
    "2. å®ç°åŠ¨æ€ä»“ä½ç®¡ç†è€Œéå›ºå®šæ»¡ä»“\n",
    "3. æ•´åˆæ³¨æ„åŠ›æœºåˆ¶å¤„ç†æ—¶é—´åºåˆ—\n",
    "4. å°è¯•Rainbow DQNæ•´åˆå¤šç§æŠ€æœ¯\n",
    "\n",
    "---\n",
    "\n",
    "**é¡¹ç›®å®Œæˆï¼** ğŸ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
